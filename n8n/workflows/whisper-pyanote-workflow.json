{
  "name": "Whisper + Pyannote Complete - Fixed Endpoints",
  "nodes": [
    {
      "parameters": {
        "path": "webhook-audio-complete",
        "options": {}
      },
      "id": "webhook-upload",
      "name": "Webhook Upload",
      "type": "n8n-nodes-base.webhook",
      "typeVersion": 1,
      "position": [240, 300]
    },
    {
      "parameters": {
        "jsCode": "// Extract and prepare audio data\nconst inputData = $input.all()[0];\n\nconsole.log('ðŸ“¥ Processing audio upload for complete pipeline');\n\nlet audioData = null;\nlet meetingId = parseInt(inputData.body?.meeting_id || '1');\nlet userId = parseInt(inputData.body?.user_id || '1');\nlet filename = inputData.body?.filename || 'uploaded-audio.wav';\n\n// Find audio data using multiple methods\nif (inputData.binary && inputData.binary.data) {\n  audioData = inputData.binary.data;\n} else {\n  function findAudioData(obj) {\n    for (const [key, value] of Object.entries(obj)) {\n      if (Buffer.isBuffer(value)) {\n        console.log(`âœ… Found Buffer at ${key}, size: ${value.length}`);\n        return value;\n      }\n      if (value && typeof value === 'object' && !Array.isArray(value)) {\n        const result = findAudioData(value);\n        if (result) return result;\n      }\n    }\n    return null;\n  }\n  audioData = findAudioData(inputData);\n}\n\nif (!audioData) {\n  throw new Error('No audio data found in upload');\n}\n\nconsole.log('ðŸŽµ Audio data prepared:', {\n  size: audioData.length,\n  meetingId: meetingId,\n  userId: userId,\n  filename: filename\n});\n\n// Return three paths: Whisper, Pyannote, and original data\nreturn [\n  // Path 1: Whisper transcription\n  {\n    json: {\n      process_type: 'whisper',\n      meeting_id: meetingId,\n      user_id: userId,\n      filename: filename,\n      timestamp: new Date().toISOString()\n    },\n    binary: { data: audioData }\n  },\n  // Path 2: Pyannote speaker identification\n  {\n    json: {\n      process_type: 'pyannote',\n      meeting_id: meetingId,\n      user_id: userId,\n      filename: filename,\n      session_id: `session_${meetingId}_${Date.now()}`,\n      timestamp: new Date().toISOString()\n    },\n    binary: { data: audioData }\n  },\n  // Path 3: Original data for later use\n  {\n    json: {\n      process_type: 'original',\n      meeting_id: meetingId,\n      user_id: userId,\n      filename: filename,\n      timestamp: new Date().toISOString()\n    },\n    binary: { data: audioData }\n  }\n];"
      },
      "id": "prepare-audio-data",
      "name": "Prepare Audio Data",
      "type": "n8n-nodes-base.code",
      "typeVersion": 2,
      "position": [460, 300]
    },
    {
      "parameters": {
        "url": "https://mayo-openai.openai.azure.com/openai/deployments/Whisper/audio/transcriptions?api-version=2024-06-01",
        "method": "POST",
        "sendHeaders": true,
        "headerParameters": {
          "parameters": [
            {
              "name": "api-key",
              "value": "4Ql0ksabF9zNgntb9RyCJIBJoDY9OKmwNTkUEcFQo3VANHE5vQa8JQQJ99BEAC5RqLJXJ3w3AAABACOG7eGl"
            }
          ]
        },
        "sendBody": true,
        "contentType": "multipart-form-data",
        "bodyParameters": {
          "parameters": [
            {
              "name": "file",
              "value": "",
              "parameterType": "formBinaryData",
              "inputDataFieldName": "data"
            },
            {
              "name": "model",
              "value": "whisper-1"
            },
            {
              "name": "language",
              "value": "nl"
            },
            {
              "name": "response_format",
              "value": "verbose_json"
            },
            {
              "name": "timestamp_granularities[]",
              "value": "word"
            }
          ]
        }
      },
      "id": "azure-whisper",
      "name": "Azure Whisper",
      "type": "n8n-nodes-base.httpRequest",
      "typeVersion": 4.1,
      "position": [680, 200]
    },
    {
      "parameters": {
        "jsCode": "// Call pyannote identify-speakers endpoint\nconst audioData = $binary.data;\nconst requestInfo = $json;\n\nconsole.log('ðŸŽ¯ Calling pyannote identify-speakers endpoint');\n\n// Convert audio to base64 for pyannote\nconst base64Audio = audioData.toString('base64');\n\nconsole.log('ðŸ“Š Pyannote request prepared:', {\n  session_id: requestInfo.session_id,\n  audio_size: audioData.length,\n  base64_length: base64Audio.length\n});\n\n// Use axios for HTTP request\nconst axios = require('axios');\nconst querystring = require('querystring');\n\ntry {\n  // Prepare form data for pyannote\n  const formData = querystring.stringify({\n    audio_data: base64Audio,\n    session_id: requestInfo.session_id,\n    format: 'wav'\n  });\n  \n  const response = await axios.post('http://host.docker.internal:8001/identify-speakers', formData, {\n    headers: {\n      'Content-Type': 'application/x-www-form-urlencoded'\n    },\n    timeout: 120000 // 2 minutes timeout\n  });\n  \n  console.log('âœ… Pyannote response received:', {\n    success: response.data.success,\n    total_speakers: response.data.total_speakers,\n    primary_speaker: response.data.primary_speaker,\n    total_duration: response.data.total_duration\n  });\n  \n  // Transform pyannote response to diarization format\n  const segments = response.data.speaker_segments?.map(segment => ({\n    start: segment.start || 0,\n    end: segment.end || segment.duration || 30,\n    speaker: segment.speaker || 'SPEAKER_1',\n    confidence: segment.confidence || 0.5\n  })) || [];\n  \n  return {\n    success: true,\n    segments: segments,\n    num_speakers: response.data.total_speakers || 1,\n    duration: response.data.total_duration || 30,\n    primary_speaker: response.data.primary_speaker,\n    pyannote_raw: response.data\n  };\n  \n} catch (error) {\n  console.log('âŒ Pyannote request failed:', error.message);\n  \n  // Return fallback response\n  return {\n    success: false,\n    error: error.message,\n    segments: [\n      {\n        start: 0,\n        end: 30,\n        speaker: 'Onbekende Spreker',\n        confidence: 0.0\n      }\n    ],\n    num_speakers: 1,\n    duration: 30\n  };\n}"
      },
      "id": "pyannote-identify-speakers",
      "name": "Pyannote Identify Speakers",
      "type": "n8n-nodes-base.code",
      "typeVersion": 2,
      "position": [680, 400]
    },
    {
      "parameters": {
        "mode": "combine",
        "combinationMode": "mergeByPosition",
        "options": {}
      },
      "id": "merge-whisper-pyannote",
      "name": "Merge Whisper + Pyannote",
      "type": "n8n-nodes-base.merge",
      "typeVersion": 2.1,
      "position": [900, 300]
    },
    {
      "parameters": {
        "jsCode": "// Combine Whisper transcription with Pyannote speaker identification\nconst whisperData = $('Azure Whisper').item.json;\nconst pyannoteData = $('Pyannote Identify Speakers').item.json;\nconst originalInfo = $('Prepare Audio Data').item.json;\n\nconsole.log('ðŸ”„ Combining Whisper + Pyannote results for database storage');\nconsole.log('ðŸ“ Whisper text length:', whisperData.text?.length || 0);\nconsole.log('ðŸ‘¥ Pyannote speakers found:', pyannoteData.num_speakers || 0);\n\n// Speaker color mapping\nconst speakerColors = {\n  'SPEAKER_1': '#3B82F6',\n  'SPEAKER_2': '#EF4444',\n  'SPEAKER_3': '#10B981', \n  'SPEAKER_4': '#F59E0B',\n  'SPEAKER_5': '#8B5CF6',\n  'Onbekende Spreker': '#6B7280',\n  'Spreker 1': '#3B82F6',\n  'Spreker 2': '#EF4444'\n};\n\n// Convert pyannote speaker names to friendly Dutch names\nfunction getFriendlySpeakerName(pyannoteId) {\n  const mapping = {\n    'SPEAKER_1': 'Spreker 1',\n    'SPEAKER_2': 'Spreker 2', \n    'SPEAKER_3': 'Spreker 3',\n    'SPEAKER_4': 'Spreker 4',\n    'SPEAKER_5': 'Spreker 5'\n  };\n  return mapping[pyannoteId] || pyannoteId || 'Onbekende Spreker';\n}\n\n// Function to assign speakers to words based on timing\nfunction assignSpeakersToWords(words, speakerSegments) {\n  if (!words || !speakerSegments || speakerSegments.length === 0) {\n    // No speaker info or words, assign all to single speaker\n    return words?.map(word => ({\n      ...word,\n      speaker: 'Spreker 1',\n      speaker_confidence: 0.5\n    })) || [];\n  }\n  \n  return words.map(word => {\n    const wordTime = word.start;\n    \n    // Find which speaker segment this word belongs to\n    const speakerSegment = speakerSegments.find(segment => \n      wordTime >= segment.start && wordTime <= segment.end\n    );\n    \n    return {\n      ...word,\n      speaker: speakerSegment ? getFriendlySpeakerName(speakerSegment.speaker) : 'Spreker 1',\n      speaker_confidence: speakerSegment?.confidence || 0.5\n    };\n  });\n}\n\n// Function to create speaker-based text segments\nfunction createSpeakerSegments(words) {\n  if (!words || words.length === 0) {\n    // No word-level data, create single segment with full text\n    return [{\n      speaker: 'Spreker 1',\n      speaker_confidence: 0.8,\n      start_time: 0,\n      end_time: whisperData.duration || 30,\n      words: [],\n      text: whisperData.text || 'Geen transcriptie beschikbaar'\n    }];\n  }\n  \n  const segments = [];\n  let currentSegment = null;\n  \n  words.forEach(word => {\n    if (!currentSegment || currentSegment.speaker !== word.speaker) {\n      // Save previous segment\n      if (currentSegment) {\n        segments.push(currentSegment);\n      }\n      \n      // Start new segment\n      currentSegment = {\n        speaker: word.speaker,\n        speaker_confidence: word.speaker_confidence,\n        start_time: word.start,\n        end_time: word.end,\n        words: [word.word],\n        text: word.word\n      };\n    } else {\n      // Continue current segment\n      currentSegment.end_time = word.end;\n      currentSegment.words.push(word.word);\n      currentSegment.text += ' ' + word.word;\n    }\n  });\n  \n  // Add final segment\n  if (currentSegment) {\n    segments.push(currentSegment);\n  }\n  \n  return segments;\n}\n\n// Process the data\nconst wordsWithSpeakers = assignSpeakersToWords(\n  whisperData.words || [], \n  pyannoteData.segments || []\n);\n\nconst speakerSegments = createSpeakerSegments(wordsWithSpeakers);\n\n// Prepare transcription records for database\nconst transcriptionRecords = speakerSegments.map((segment, index) => {\n  const now = new Date();\n  const spokenAt = new Date(now.getTime() + (segment.start_time * 1000));\n  \n  return {\n    meeting_id: originalInfo.meeting_id,\n    text: segment.text.trim().replace(/'/g, \"''\"), // Escape quotes for SQL\n    speaker_name: segment.speaker,\n    speaker_color: speakerColors[segment.speaker] || '#6B7280',\n    confidence: 0.95, // High confidence for Whisper\n    source: 'n8n_whisper_pyannote_complete',\n    is_final: true,\n    spoken_at: spokenAt.toISOString(),\n    created_at: now.toISOString(),\n    updated_at: now.toISOString(),\n    metadata: JSON.stringify({\n      segment_index: index,\n      start_time: segment.start_time,\n      end_time: segment.end_time,\n      word_count: segment.words.length,\n      speaker_confidence: segment.speaker_confidence,\n      processing_method: 'whisper_pyannote_complete',\n      whisper_language: whisperData.language,\n      speaker_detection_method: 'pyannote_identify_speakers',\n      original_filename: originalInfo.filename,\n      processed_at: now.toISOString(),\n      whisper_duration: whisperData.duration,\n      pyannote_speakers: pyannoteData.num_speakers,\n      pyannote_primary_speaker: pyannoteData.primary_speaker\n    }).replace(/'/g, \"''\")\n  };\n});\n\nconsole.log('âœ… Data prepared for PostgreSQL insertion:', {\n  transcription_records: transcriptionRecords.length,\n  meeting_id: originalInfo.meeting_id,\n  speakers_detected: pyannoteData.num_speakers\n});\n\nreturn {\n  meeting_id: originalInfo.meeting_id,\n  user_id: originalInfo.user_id,\n  transcription_records: transcriptionRecords,\n  processing_summary: {\n    original_filename: originalInfo.filename,\n    whisper_text_length: whisperData.text?.length || 0,\n    whisper_language: whisperData.language,\n    audio_duration: whisperData.duration,\n    speakers_detected: pyannoteData.num_speakers || 1,\n    primary_speaker: pyannoteData.primary_speaker,\n    segments_created: transcriptionRecords.length,\n    words_processed: wordsWithSpeakers.length,\n    processed_at: new Date().toISOString(),\n    pyannote_success: pyannoteData.success\n  }\n};"
      },
      "id": "combine-data",
      "name": "Combine Data",
      "type": "n8n-nodes-base.code",
      "typeVersion": 2,
      "position": [1120, 300]
    },
    {
      "parameters": {
        "operation": "executeQuery",
        "query": "=INSERT INTO transcriptions (\n  meeting_id, text, speaker_name, speaker_color, confidence, source, is_final, spoken_at, created_at, updated_at, metadata\n) VALUES \n{{ $json.transcription_records.map(record => \n  `(${record.meeting_id}, '${record.text}', '${record.speaker_name}', '${record.speaker_color}', ${record.confidence}, '${record.source}', ${record.is_final}, '${record.spoken_at}', '${record.created_at}', '${record.updated_at}', '${record.metadata}')`\n).join(',\\n') }}",
        "options": {}
      },
      "id": "insert-transcriptions",
      "name": "Insert Transcriptions",
      "type": "n8n-nodes-base.postgres",
      "typeVersion": 2.4,
      "position": [1340, 300],
      "credentials": {
        "postgres": {
          "name": "ConversationHub PostgreSQL"
        }
      }
    },
    {
      "parameters": {
        "operation": "executeQuery",
        "query": "=INSERT INTO meeting_reports (\n  meeting_id,\n  report_title,\n  report_content,\n  report_type,\n  generated_by,\n  generated_at,\n  created_at,\n  updated_at,\n  metadata\n) VALUES (\n  {{ $('Combine Data').item.json.meeting_id }},\n  'Complete Audio Analysis - {{ $('Combine Data').item.json.processing_summary.original_filename }}',\n  'Automatisch gegenereerd verslag met speaker identificatie:\\n\\nðŸ“Š Verwerkingsstatistieken:\\n- Bestand: {{ $('Combine Data').item.json.processing_summary.original_filename }}\\n- Duur: {{ $('Combine Data').item.json.processing_summary.audio_duration }} seconden\\n- Taal: {{ $('Combine Data').item.json.processing_summary.whisper_language }}\\n- Sprekers gedetecteerd: {{ $('Combine Data').item.json.processing_summary.speakers_detected }}\\n- Primaire spreker: {{ $('Combine Data').item.json.processing_summary.primary_speaker }}\\n- Transcriptie segmenten: {{ $('Combine Data').item.json.processing_summary.segments_created }}\\n- Woorden verwerkt: {{ $('Combine Data').item.json.processing_summary.words_processed }}\\n\\nðŸŽ¤ Transcriptie met Sprekers:\\n{{ $('Combine Data').item.json.transcription_records.map(record => `${record.speaker_name}: ${record.text}`).join(\"\\n\\n\") }}',\n  'ai_complete_analysis',\n  'N8N_Whisper_Pyannote_Complete',\n  NOW(),\n  NOW(),\n  NOW(),\n  '{{ JSON.stringify($('Combine Data').item.json.processing_summary).replace(/'/g, \"''\") }}'\n)",
        "options": {}
      },
      "id": "create-report",
      "name": "Create Report",
      "type": "n8n-nodes-base.postgres",
      "typeVersion": 2.4,
      "position": [1560, 300],
      "credentials": {
        "postgres": {
          "name": "ConversationHub PostgreSQL"
        }
      }
    },
    {
      "parameters": {
        "respondWith": "json",
        "responseBody": "={\n  \"success\": true,\n  \"message\": \"Complete audio analysis completed successfully\",\n  \"data\": {\n    \"meeting_id\": {{ $('Combine Data').item.json.meeting_id }},\n    \"transcriptions_created\": {{ $('Combine Data').item.json.transcription_records.length }},\n    \"speakers_detected\": {{ $('Combine Data').item.json.processing_summary.speakers_detected }},\n    \"primary_speaker\": \"{{ $('Combine Data').item.json.processing_summary.primary_speaker }}\",\n    \"audio_duration\": {{ $('Combine Data').item.json.processing_summary.audio_duration }},\n    \"whisper_language\": \"{{ $('Combine Data').item.json.processing_summary.whisper_language }}\",\n    \"processing_time\": \"{{ $('Combine Data').item.json.processing_summary.processed_at }}\",\n    \"pyannote_success\": {{ $('Combine Data').item.json.processing_summary.pyannote_success }},\n    \"report_created\": true\n  }\n}"
      },
      "id": "final-response",
      "name": "Final Response",
      "type": "n8n-nodes-base.respondToWebhook",
      "typeVersion": 1,
      "position": [1780, 300]
    }
  ],
  "connections": {
    "Webhook Upload": {
      "main": [
        [
          {
            "node": "Prepare Audio Data",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Prepare Audio Data": {
      "main": [
        [
          {
            "node": "Azure Whisper",
            "type": "main",
            "index": 0
          }
        ],
        [
          {
            "node": "Pyannote Identify Speakers",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Azure Whisper": {
      "main": [
        [
          {
            "node": "Merge Whisper + Pyannote",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Pyannote Identify Speakers": {
      "main": [
        [
          {
            "node": "Merge Whisper + Pyannote",
            "type": "main",
            "index": 1
          }
        ]
      ]
    },
    "Merge Whisper + Pyannote": {
      "main": [
        [
          {
            "node": "Combine Data",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Combine Data": {
      "main": [
        [
          {
            "node": "Insert Transcriptions",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Insert Transcriptions": {
      "main": [
        [
          {
            "node": "Create Report",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Create Report": {
      "main": [
        [
          {
            "node": "Final Response",
            "type": "main",
            "index": 0
          }
        ]
      ]
    }
  }
}